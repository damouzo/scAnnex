/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Apocrita HPC (QMUL) Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Optimized for Queen Mary University of London's Apocrita SLURM cluster
    
    Documentation: https://slurm-docs.hpc.qmul.ac.uk/
    Support: its-research-support@qmul.ac.uk
    
    Usage: 
      nextflow run main.nf -profile apocrita,singularity  # Recommended for production
      nextflow run main.nf -profile apocrita,conda        # Alternative with conda
    
    IMPORTANT STORAGE NOTES:
      - /data/home/$USER: 100GB limit, permanent (use for small results <100GB)
      - /gpfs/scratch/$USER: 3TB quota, auto-delete after 65 days (use for work-dir)
      - Work directory: Automatically set to /gpfs/scratch/$USER/scannex_work
      - Output directory: YOU choose via --outdir (recommend /data/home for <100GB)
----------------------------------------------------------------------------------------
*/

// Inherit generic SLURM configuration
includeConfig 'slurm.config'

// CRITICAL: Set work directory to scratch (3TB quota, auto-cleanup)
// This ensures intermediate/temporary files don't fill up home directory (100GB limit)
// Users can override with: -work-dir /custom/path
workDir = "/gpfs/scratch/${System.getenv('USER')}/scannex_work"

// Apocrita-specific parameters
params {
    // Override SLURM defaults for Apocrita
    // Apocrita auto-assigns partitions, no need to specify
    slurm_queue = null
    
    // IMPORTANT: Most Apocrita users need to use the 'pilot' account
    // Contact its-research-support@qmul.ac.uk to get access
    slurm_account = 'pilot'
}

process {
    // CRITICAL: Use scratch for temporary files
    // Scratch has 3TB quota, home has only 100GB
    // Files in scratch are auto-deleted after 65 days
    scratch = "/gpfs/scratch/${System.getenv('USER')}/scannex_tmp"
    
    // Apocrita-optimized resource allocations
    // Conservative values for shared node environment
    // Based on Apocrita documentation: https://slurm-docs.hpc.qmul.ac.uk/nodes/
    
    withLabel: 'process_single' {
        cpus = 1
        memory = { 6.GB * task.attempt }
        time = { 4.h * task.attempt }
    }
    
    withLabel: 'process_low' {
        cpus = 2
        memory = { 12.GB * task.attempt }
        time = { 4.h * task.attempt }
    }
    
    withLabel: 'process_medium' {
        cpus = 6
        memory = { 24.GB * task.attempt }  // Conservative for Apocrita shared nodes
        time = { 8.h * task.attempt }
    }
    
    withLabel: 'process_high' {
        cpus = 12
        memory = { 48.GB * task.attempt }  // Conservative for Apocrita shared nodes
        time = { 16.h * task.attempt }
    }
    
    withLabel: 'process_long' {
        time = { 24.h * task.attempt }
    }
    
    withLabel: 'process_high_memory' {
        memory = { 96.GB * task.attempt }
        time = { 12.h * task.attempt }
    }
}

// NOTE: Software manager (conda/singularity) is controlled by profile combination:
//   -profile apocrita,conda → uses conda (conda profile from nextflow.config)
//   -profile apocrita,singularity → uses singularity (singularity profile from nextflow.config)
//
// Singularity cache configuration for Apocrita (when singularity profile is active)
// This only sets cache location, not enabling - that's done by singularity profile
// Singularity cache configuration for Apocrita
// Cache location only - enabling is done by -profile singularity
singularity.cacheDir = "/gpfs/scratch/${System.getenv('USER')}/singularity_cache"
singularity.autoMounts = true
singularity.runOptions = '--no-home'

// Disable other container engines
docker.enabled = false
podman.enabled = false
apptainer.enabled = false
shifter.enabled = false
charliecloud.enabled = false

// Environment variables specific to Apocrita
env {
    // Ensure Singularity uses scratch for cache
    SINGULARITY_CACHEDIR = "/gpfs/scratch/${System.getenv('USER')}/singularity_cache"
    
    // Prevent local Python/R library conflicts
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER = "/.Rprofile"
    R_ENVIRON_USER = "/.Renviron"
}
